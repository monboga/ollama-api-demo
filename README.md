# Ollama API Demo con Spring Boot

Este proyecto es una demostraci√≥n de c√≥mo integrar y consumir un modelo de lenguaje de **Ollama** utilizando una API REST construida con **Spring Boot**. El entorno est√° completamente orquestado con **Docker Compose**, incluyendo un script de inicio que descarga y configura autom√°ticamente el modelo de lenguaje, dejando el servicio listo para ser consumido al instante.

## ‚ú® Caracter√≠sticas

* **API REST para interactuar con Ollama:** Expone un endpoint sencillo para enviar prompts y recibir respuestas del modelo de lenguaje.
* **Entorno 100% Dockerizado:** La aplicaci√≥n, el servidor de Ollama y sus dependencias se gestionan a trav√©s de Docker, garantizando un despliegue consistente y aislado.
* **Modelo de IA Pre-cargado:** El modelo de lenguaje (ej. `llama3.2:1b`) se descarga autom√°ticamente la primera vez que se inician los servicios, gracias a un script de inicio personalizado.
* **Comunicaci√≥n Optimizada:** Los contenedores se comunican a trav√©s de una red Docker dedicada para mayor seguridad y fiabilidad.

## üìÅ Estructura del Proyecto

El proyecto est√° organizado para separar la configuraci√≥n de cada servicio:

```
.
‚îú‚îÄ‚îÄ ollama/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile      # Define la imagen personalizada de Ollama
‚îÇ   ‚îî‚îÄ‚îÄ entrypoint.sh   # Script que descarga el modelo al iniciar el contenedor
‚îú‚îÄ‚îÄ src/                # C√≥digo fuente de la aplicaci√≥n Spring Boot
‚îú‚îÄ‚îÄ Dockerfile          # Define la imagen de la aplicaci√≥n Spring Boot
‚îú‚îÄ‚îÄ docker-compose.yml  # Orquesta todos los servicios (app y ollama)
‚îî‚îÄ‚îÄ README.md           # Esta documentaci√≥n
```

## üìã Requisitos Previos

* **Docker** instalado en tu sistema.
* **Docker Compose** instalado (generalmente viene con Docker Desktop).

## üöÄ Configuraci√≥n y Ejecuci√≥n

Sigue estos pasos para levantar todo el entorno.

### 1. Clona el Repositorio
```bash
git clone git@github.com:monboga/ollama-api-demo.git
cd ollama-api-demo
```

### 2. Construye las Im√°genes de Docker
Este comando leer√° los `Dockerfile` y el `docker-compose.yml` para construir las im√°genes personalizadas de tu aplicaci√≥n y de Ollama.

**Nota:** La primera vez que ejecutes este comando, puede tardar varios minutos, ya que descargar√° el modelo de lenguaje definido en `ollama/entrypoint.sh`.

```bash
docker-compose build
```

### 3. Inicia los Servicios
Este comando iniciar√° los contenedores de la API de Spring Boot y de Ollama en segundo plano.
```bash
docker-compose up -d
```
El backend estar√° listo para recibir peticiones una vez que los contenedores se hayan iniciado.

### 4. Verifica que Todo Funcione (Opcional pero Recomendado)

* **Verifica que el modelo de Ollama est√© disponible:**
    ```bash
    docker exec -it ollama ollama list
    ```
    Deber√≠as ver el modelo (ej. `llama3.2:1b`) en la lista.

* **Prueba la API directamente:**
    Puedes usar `curl` en tu terminal o simplemente pegar la URL en un navegador.
    ```bash
    curl "http://localhost:8080/api/v1/generate?promptMessage=Hola, pres√©ntate en una frase."
    ```
    Deber√≠as recibir una respuesta en texto generada por el modelo de IA.

### 5. Det√©n los Servicios
Cuando termines de usar la aplicaci√≥n, puedes detener y eliminar los contenedores con:
```bash
docker-compose down
```

## üîß Personalizaci√≥n

### Cambiar el Modelo de IA

Para usar un modelo de Ollama diferente, debes actualizar su nombre en **dos** lugares:

1.  **Script de descarga:** En el archivo `ollama/entrypoint.sh`, modifica la l√≠nea `ollama pull nombre-del-modelo`.
2.  **C√≥digo de Spring Boot:** En el archivo `src/main/java/edu/labvr/ollama_api_demo/service/LlamaAIService.java`, modifica la l√≠nea `options.setModel("nombre-del-modelo");`.

Despu√©s de hacer estos cambios, recuerda reconstruir las im√°genes con `docker-compose build`.

## üì° Endpoint de la API

* **URL:** `http://localhost:8080/api/v1/generate`
* **M√©todo:** `GET`
* **Par√°metro:** `promptMessage` (string)
* **Ejemplo de uso:**
    ```
    http://localhost:8080/api/v1/generate?promptMessage=Explica qu√© es una red neuronal en 20 palabras.
    ```